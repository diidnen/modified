{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609b7aa9-de49-4db4-9e3f-8f9a35432acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd814301ffb8419ebd1314d5be60240d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcf49ab496d4174938a7388dd25e39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['name', 'gexp', 'image', 'source', 'tissue'])\n",
      "Image input shape: torch.Size([4, 3, 224, 224])\n",
      "GEXP input shape: torch.Size([4, 19266])\n",
      "Successfully loaded weights for uni\n",
      "{'mask_gene_name': False, 'gene_num': 19266, 'seq_len': 19266, 'encoder': {'hidden_dim': 768, 'depth': 12, 'heads': 12, 'dim_head': 64, 'seq_len': 19266, 'module_type': 'transformer', 'norm_first': False}, 'decoder': {'hidden_dim': 512, 'depth': 6, 'heads': 8, 'dim_head': 64, 'module_type': 'performer', 'seq_len': 19266, 'norm_first': False}, 'n_class': 104, 'pad_token_id': 103, 'mask_token_id': 102, 'bin_num': 100, 'bin_alpha': 1.0, 'rawcount': True, 'model': 'mae_autobin', 'test_valid_train_idx_dict': '/nfs_beijing/minsheng/data/os10000w-new/global_shuffle/meta.csv.train_set_idx_dict.pt', 'valid_data_path': '/nfs_beijing/minsheng/data/valid_count_10w.npz', 'num_tokens': 13, 'train_data_path': None, 'isPanA': False, 'isPlanA1': False, 'max_files_to_load': 5, 'bin_type': 'auto_bin', 'value_mask_prob': 0.3, 'zero_mask_prob': 0.03, 'replace_prob': 0.8, 'random_token_prob': 0.1, 'mask_ignore_token_ids': [0], 'decoder_add_zero': True, 'mae_encoder_max_seq_len': 15000, 'isPlanA': False, 'mask_prob': 0.3, 'model_type': 'mae_autobin', 'pos_embed': False, 'device': 'cuda'}\n",
      "Successfully loaded weights for scFoundation\n",
      "uni trainable params: 18.11M || all params: 321.46M || trainable%: 5.633831418677185\n",
      "scFoundation trainable params: 0.10M || all params: 100.04M || trainable%: 0.09839252098488002\n",
      "Image Embedding Dimension: torch.Size([4, 3, 224, 224])\n",
      "Gene Expression Embedding Dimension: torch.Size([4, 128])\n",
      "Logit Scale: 14.29\n",
      "\n",
      "=== Reproducing the Bug in compute_loss ===\n",
      "Reproduced Error: mat1 and mat2 shapes cannot be multiplied (2688x224 and 128x4)\n",
      "Img embed shape in error context: torch.Size([4, 3, 224, 224])\n",
      "Gexp embed shape in error context: torch.Size([4, 128])\n",
      "\n",
      "=== Simulating ClipLoss Matmul ===\n",
      "Matmul Error: mat1 and mat2 shapes cannot be multiplied (2688x224 and 128x4)\n",
      " - Img for matmul: torch.Size([4, 3, 224, 224]) (rows x cols)\n",
      " - Gexp.T for matmul: torch.Size([128, 4]) (cols x rows)\n",
      "Inner dims mismatch: 224 != 128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hescape.data_modules.image_gexp_dataset import ImageGexpDataModule\n",
    "from hescape.constants import DatasetEnum\n",
    "from hescape.models.clip import CLIPModel\n",
    "from torch.distributed import get_rank  # For rank fallback\n",
    "\n",
    "# Full config_dict (from your original YAML, complete datamodule + model + paths)\n",
    "config_dict = {\n",
    "    \"devices_per_job\": 4,\n",
    "    \"model\": {\n",
    "        \"litmodule\": {\n",
    "            \"_target_\": \"hescape.modules.pretrain_module.PretrainModule\",\n",
    "            \"_partial_\": True,\n",
    "            \"input_genes\": 343,\n",
    "            \"embed_dim\": 128,\n",
    "            \"img_enc_name\": \"uni\",\n",
    "            \"gene_enc_name\": \"scFoundation\",\n",
    "            \"loss\": \"CLIP\",\n",
    "            \"img_finetune\": True,\n",
    "            \"gene_finetune\": False,\n",
    "            \"img_proj\": \"moe\",  # Test moe\n",
    "            \"gene_proj\": \"linear\",\n",
    "            \"n_tissue\": None,\n",
    "            \"n_region\": None,\n",
    "            \"image_size\": 224,\n",
    "            \"temperature\": 0.07,\n",
    "            \"lr\": 1.0e-05,\n",
    "            \"weight_decay\": 0.01\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"lr\": 1.0e-05,\n",
    "            \"weight_decay\": 0.01\n",
    "        },\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"anatomy\": {\n",
    "            \"dataset_name\": \"human-lung-healthy-panel\",\n",
    "            \"train_csv\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/UCFhescape/data/human-lung-healthy-panel/train.csv\",\n",
    "            \"val_csv\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/UCFhescape/data/human-lung-healthy-panel/val.csv\",\n",
    "            \"test_csv\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/UCFhescape/data/human-lung-healthy-panel/test.csv\",\n",
    "            \"output\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/UCFhescape/../results/human_lung_healthy_panel/local\",\n",
    "            \"pretrain_weights\": {\n",
    "                \"drvi_model_dir\": \"drvi_human_lung_healthy_panel\",\n",
    "                \"data_gene_reference_path\": \"../human_lung_healthy_panel/nicheformer_reference.h5ad\"\n",
    "            }\n",
    "        },\n",
    "        \"dataset_path\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/UCFhescape/human_lung_panel\",\n",
    "        \"data_files_path\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/UCFhescape/data\",\n",
    "        \"base_output_path\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/UCFhescape/../results\",\n",
    "        \"pretrain_weights\": {\n",
    "            \"img_enc_path\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/pretrain_weights/image\",\n",
    "            \"gene_enc_path\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/pretrain_weights/gene\",\n",
    "            \"base_data_gene_reference_path\": \"/p/project1/hai_spatial_clip/hescape/data\"\n",
    "        }\n",
    "    },\n",
    "    \"datamodule\": {\n",
    "        \"_target_\": \"hescape.data_modules.image_gexp_dataset.ImageGexpDataModule\",\n",
    "        \"dataset_path\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/UCFhescape/human_lung_panel\",\n",
    "        \"dataset_name\": \"human-lung-healthy-panel\",\n",
    "        \"data_files_path\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/UCFhescape/data\",\n",
    "        \"img_model_name\": \"uni\",\n",
    "        \"gene_model_name\": \"scFoundation\",\n",
    "        \"num_workers\": 0,  # Single GPU, no workers\n",
    "        \"pin_memory\": True,\n",
    "        \"persistent_workers\": False,  # Single, no persistent\n",
    "        \"batch_size\": 4,  # Small for test\n",
    "        \"source_key\": \"tissue\",\n",
    "        \"source_value\": [\"lung\"],\n",
    "        \"split_key\": \"name\",\n",
    "        \"split_train_csv\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/UCFhescape/data/human-lung-healthy-panel/train.csv\",\n",
    "        \"split_val_csv\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/UCFhescape/data/human-lung-healthy-panel/val.csv\",\n",
    "        \"split_test_csv\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/UCFhescape/data/human-lung-healthy-panel/test.csv\",\n",
    "        \"input_genes\": 343,\n",
    "        \"seed\": 24442,\n",
    "        \"data_gene_reference_path\": \"/lus/lfs1aip1/home/u5t/chuhansong.u5t/UCFhescape/data/human-lung-healthy-panel/nicheformer_reference.h5ad\"\n",
    "    },\n",
    "    \"name\": \"hescape_default_training\"\n",
    "}\n",
    "\n",
    "cfg = OmegaConf.create(config_dict)\n",
    "\n",
    "# Extract datamodule config\n",
    "dm_config = cfg.datamodule\n",
    "\n",
    "# Instantiate and setup datamodule (single GPU, small batch)\n",
    "dm = ImageGexpDataModule(\n",
    "    dataset_path=dm_config.dataset_path,\n",
    "    dataset_name=dm_config.dataset_name,\n",
    "    data_files_path=dm_config.data_files_path,\n",
    "    img_model_name=dm_config.img_model_name,\n",
    "    gene_model_name=dm_config.gene_model_name,\n",
    "    num_workers=dm_config.num_workers,  # 0 for single\n",
    "    pin_memory=dm_config.pin_memory,\n",
    "    persistent_workers=dm_config.persistent_workers,  # False for single\n",
    "    batch_size=dm_config.batch_size,  # 4 small\n",
    "    source_key=dm_config.source_key,\n",
    "    source_value=dm_config.source_value,\n",
    "    split_key=dm_config.split_key,\n",
    "    split_train_csv=dm_config.split_train_csv,\n",
    "    split_val_csv=dm_config.split_val_csv,\n",
    "    split_test_csv=dm_config.split_test_csv,\n",
    "    input_genes=dm_config.input_genes,\n",
    "    seed=dm_config.seed,\n",
    "    data_gene_reference_path=dm_config.data_gene_reference_path  # This is the key arg\n",
    ")\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup('fit')  # Setup for training\n",
    "\n",
    "# Get one batch from train loader\n",
    "train_loader = dm.train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch keys: {batch.keys()}\")\n",
    "print(f\"Image input shape: {batch[DatasetEnum.IMG].shape}\")\n",
    "print(f\"GEXP input shape: {batch[DatasetEnum.GEXP].shape}\")\n",
    "\n",
    "# Extract model params from config (litmodule section)\n",
    "model_params = cfg.model.litmodule\n",
    "model_params.world_size = 1  # Single process test\n",
    "model_params.rank = 0  # For single process test\n",
    "model_params.img_enc_path = cfg.paths.pretrain_weights.img_enc_path\n",
    "model_params.gene_enc_path = cfg.paths.pretrain_weights.gene_enc_path\n",
    "model_params.drvi_model_dir = cfg.paths.anatomy.pretrain_weights.drvi_model_dir\n",
    "model_params.cfg = cfg  # Pass full config if needed\n",
    "\n",
    "# Instantiate CLIPModel with modified img_proj=\"moe\"\n",
    "model = CLIPModel(\n",
    "    input_genes=model_params.input_genes,\n",
    "    embed_dim=model_params.embed_dim,\n",
    "    img_enc_name=model_params.img_enc_name,\n",
    "    gene_enc_name=model_params.gene_enc_name,\n",
    "    loss=model_params.loss,\n",
    "    img_finetune=model_params.img_finetune,\n",
    "    gene_finetune=model_params.gene_finetune,\n",
    "    img_proj=model_params.img_proj,  # Now \"moe\"\n",
    "    gene_proj=model_params.gene_proj,\n",
    "    n_tissue=model_params.n_tissue,\n",
    "    n_region=model_params.n_region,\n",
    "    image_size=model_params.image_size,\n",
    "    temperature=model_params.temperature,\n",
    "    world_size=model_params.world_size,\n",
    "    rank=model_params.rank,\n",
    "    cfg=model_params.cfg,\n",
    "    img_enc_path=model_params.img_enc_path,\n",
    "    gene_enc_path=model_params.gene_enc_path,\n",
    "    drvi_model_dir=model_params.drvi_model_dir,\n",
    ")\n",
    "\n",
    "# Move to device (use first GPU for test)\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Forward pass on the real batch (move batch to device)\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    img_embed, gexp_embed, logit_scale = model(batch, norm=True)\n",
    "\n",
    "print(f\"Image Embedding Dimension: {img_embed.shape}\")\n",
    "print(f\"Gene Expression Embedding Dimension: {gexp_embed.shape}\")\n",
    "print(f\"Logit Scale: {logit_scale.item():.2f}\")\n",
    "\n",
    "# REPRODUCE THE BUG: Manually call compute_loss to trigger the matmul error\n",
    "print(\"\\n=== Reproducing the Bug in compute_loss ===\")\n",
    "try:\n",
    "    # This will fail with the matmul error if dims mismatch\n",
    "    loss_value = model.compute_loss(img_embed, gexp_embed)\n",
    "    print(f\"Loss computed successfully: {loss_value.item():.4f}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Reproduced Error: {str(e)}\")\n",
    "    print(f\"Img embed shape in error context: {img_embed.shape}\")\n",
    "    print(f\"Gexp embed shape in error context: {gexp_embed.shape}\")\n",
    "\n",
    "# ADDITIONAL DEBUG: Simulate the exact matmul from ClipLoss\n",
    "print(\"\\n=== Simulating ClipLoss Matmul ===\")\n",
    "try:\n",
    "    # From open_clip/loss.py: logits_per_image = logit_scale * img_features @ gexp_features.T\n",
    "    scaled_img = logit_scale * img_embed\n",
    "    logits = scaled_img @ gexp_embed.T\n",
    "    print(f\"Matmul successful: logits shape {logits.shape}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Matmul Error: {str(e)}\")\n",
    "    # If it matches the bug shapes, print more details\n",
    "    if \"mat1 and mat2 shapes cannot be multiplied\" in str(e):\n",
    "        print(f\" - Img for matmul: {img_embed.shape} (rows x cols)\")\n",
    "        print(f\" - Gexp.T for matmul: {gexp_embed.T.shape} (cols x rows)\")\n",
    "        print(f\"Inner dims mismatch: {img_embed.shape[-1]} != {gexp_embed.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2430a1e-84eb-48d8-aebc-72177c405149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hescape)",
   "language": "python",
   "name": "hescape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
