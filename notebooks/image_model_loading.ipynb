{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29059333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/project1/hai_spatial_clip/miniforge3/envs/hescape/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/p/project1/hai_spatial_clip/miniforge3/envs/hescape/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from hescape.models import CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f406c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded weights for uni\n",
      "Successfully loaded weights for nicheformer\n",
      "uni trainable params: 1.49M || all params: 304.84M || trainable%: 0.4887118439246093\n",
      "nicheformer trainable params: 0.07M || all params: 38.59M || trainable%: 0.1701735239188342\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel(\n",
    "    input_genes=280,\n",
    "    embed_dim=128,\n",
    "    img_enc_name=\"uni\",\n",
    "    gene_enc_name=\"nicheformer\",\n",
    "    loss=\"CLIP\",\n",
    "    img_finetune=True,\n",
    "    gene_finetune=False,\n",
    "    img_proj=\"mlp\",\n",
    "    gene_proj=\"linear\",\n",
    "    img_enc_path=\"/p/project1/hai_spatial_clip/pretrain_weights/image\",\n",
    "    gene_enc_path=\"/p/project1/hai_spatial_clip/pretrain_weights/gene\",\n",
    "    # drvi_model_dir=\"drvi_human_breast_panel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74edf225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (image_encoder): ImageEncoder(\n",
       "    (trunk): PeftModel(\n",
       "      (base_model): LoraModel(\n",
       "        (model): VisionTransformer(\n",
       "          (patch_embed): PatchEmbed(\n",
       "            (proj): lora.Conv2d(\n",
       "              (base_layer): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Conv2d(3, 8, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Conv2d(8, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (norm): Identity()\n",
       "          )\n",
       "          (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "          (patch_drop): Identity()\n",
       "          (norm_pre): Identity()\n",
       "          (blocks): Sequential(\n",
       "            (0): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (1): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (2): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (3): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (4): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (5): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (6): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (7): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (8): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (9): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (10): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (11): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (12): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (13): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (14): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (15): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (16): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (17): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (18): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (19): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (20): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (21): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (22): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (23): Block(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (attn): Attention(\n",
       "                (qkv): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Identity()\n",
       "                (k_norm): Identity()\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls1): LayerScale()\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (ls2): LayerScale()\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc_norm): Identity()\n",
       "          (head_drop): Dropout(p=0.0, inplace=False)\n",
       "          (head): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.2, inplace=False)\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (drop2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gexp_encoder): GexpEncoder(\n",
       "    (trunk): NicheformerModel(\n",
       "      (trunk): Nicheformer(\n",
       "        (encoder_layer): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): TransformerEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x TransformerEncoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "              (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.0, inplace=False)\n",
       "              (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (embeddings): Embedding(20345, 512, padding_idx=1)\n",
       "        (positional_embedding): Embedding(1500, 512)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss): ClipLoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88b4da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\n",
    "    \"/p/project1/hai_spatial_clip/outputs/human_breast_panel/11653143_11_11-uni-nicheformer-batch64-CLIP/checkpoints/last.ckpt\",\n",
    "    map_location=torch.device(\"cpu\"),\n",
    ")[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5ae188b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model.logit_scale', 'model.image_encoder.trunk.base_model.model.cls_token', 'model.image_encoder.trunk.base_model.model.pos_embed', 'model.image_encoder.trunk.base_model.model.patch_embed.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.patch_embed.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.patch_embed.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.patch_embed.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.0.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.0.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.0.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.0.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.0.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.0.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.0.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.0.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.0.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.0.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.0.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.0.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.0.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.0.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.0.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.0.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.0.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.0.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.1.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.1.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.1.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.1.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.1.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.1.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.1.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.1.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.1.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.1.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.1.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.1.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.1.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.1.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.1.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.1.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.1.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.1.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.2.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.2.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.2.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.2.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.2.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.2.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.2.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.2.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.2.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.2.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.2.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.2.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.2.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.2.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.2.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.2.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.2.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.2.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.3.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.3.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.3.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.3.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.3.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.3.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.3.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.3.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.3.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.3.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.3.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.3.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.3.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.3.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.3.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.3.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.3.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.3.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.4.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.4.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.4.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.4.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.4.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.4.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.4.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.4.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.4.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.4.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.4.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.4.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.4.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.4.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.4.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.4.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.4.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.4.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.5.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.5.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.5.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.5.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.5.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.5.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.5.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.5.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.5.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.5.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.5.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.5.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.5.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.5.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.5.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.5.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.5.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.5.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.6.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.6.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.6.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.6.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.6.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.6.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.6.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.6.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.6.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.6.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.6.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.6.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.6.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.6.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.6.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.6.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.6.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.6.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.7.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.7.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.7.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.7.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.7.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.7.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.7.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.7.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.7.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.7.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.7.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.7.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.7.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.7.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.7.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.7.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.7.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.7.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.8.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.8.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.8.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.8.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.8.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.8.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.8.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.8.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.8.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.8.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.8.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.8.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.8.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.8.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.8.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.8.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.8.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.8.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.9.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.9.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.9.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.9.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.9.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.9.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.9.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.9.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.9.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.9.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.9.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.9.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.9.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.9.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.9.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.9.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.9.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.9.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.10.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.10.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.10.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.10.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.10.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.10.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.10.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.10.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.10.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.10.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.10.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.10.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.10.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.10.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.10.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.10.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.10.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.10.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.11.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.11.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.11.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.11.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.11.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.11.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.11.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.11.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.11.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.11.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.11.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.11.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.11.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.11.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.11.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.11.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.11.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.11.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.12.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.12.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.12.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.12.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.12.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.12.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.12.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.12.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.12.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.12.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.12.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.12.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.12.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.12.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.12.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.12.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.12.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.12.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.13.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.13.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.13.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.13.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.13.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.13.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.13.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.13.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.13.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.13.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.13.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.13.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.13.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.13.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.13.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.13.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.13.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.13.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.14.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.14.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.14.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.14.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.14.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.14.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.14.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.14.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.14.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.14.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.14.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.14.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.14.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.14.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.14.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.14.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.14.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.14.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.15.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.15.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.15.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.15.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.15.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.15.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.15.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.15.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.15.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.15.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.15.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.15.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.15.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.15.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.15.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.15.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.15.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.15.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.16.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.16.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.16.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.16.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.16.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.16.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.16.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.16.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.16.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.16.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.16.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.16.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.16.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.16.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.16.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.16.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.16.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.16.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.17.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.17.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.17.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.17.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.17.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.17.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.17.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.17.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.17.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.17.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.17.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.17.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.17.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.17.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.17.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.17.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.17.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.17.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.18.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.18.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.18.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.18.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.18.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.18.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.18.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.18.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.18.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.18.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.18.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.18.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.18.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.18.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.18.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.18.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.18.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.18.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.19.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.19.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.19.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.19.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.19.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.19.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.19.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.19.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.19.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.19.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.19.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.19.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.19.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.19.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.19.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.19.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.19.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.19.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.20.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.20.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.20.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.20.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.20.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.20.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.20.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.20.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.20.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.20.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.20.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.20.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.20.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.20.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.20.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.20.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.20.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.20.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.21.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.21.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.21.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.21.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.21.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.21.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.21.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.21.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.21.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.21.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.21.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.21.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.21.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.21.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.21.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.21.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.21.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.21.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.22.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.22.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.22.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.22.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.22.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.22.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.22.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.22.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.22.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.22.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.22.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.22.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.22.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.22.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.22.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.22.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.22.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.22.ls2.gamma', 'model.image_encoder.trunk.base_model.model.blocks.23.norm1.weight', 'model.image_encoder.trunk.base_model.model.blocks.23.norm1.bias', 'model.image_encoder.trunk.base_model.model.blocks.23.attn.qkv.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.23.attn.qkv.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.23.attn.qkv.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.23.attn.qkv.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.23.attn.proj.base_layer.weight', 'model.image_encoder.trunk.base_model.model.blocks.23.attn.proj.base_layer.bias', 'model.image_encoder.trunk.base_model.model.blocks.23.attn.proj.lora_A.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.23.attn.proj.lora_B.default.weight', 'model.image_encoder.trunk.base_model.model.blocks.23.ls1.gamma', 'model.image_encoder.trunk.base_model.model.blocks.23.norm2.weight', 'model.image_encoder.trunk.base_model.model.blocks.23.norm2.bias', 'model.image_encoder.trunk.base_model.model.blocks.23.mlp.fc1.weight', 'model.image_encoder.trunk.base_model.model.blocks.23.mlp.fc1.bias', 'model.image_encoder.trunk.base_model.model.blocks.23.mlp.fc2.weight', 'model.image_encoder.trunk.base_model.model.blocks.23.mlp.fc2.bias', 'model.image_encoder.trunk.base_model.model.blocks.23.ls2.gamma', 'model.image_encoder.trunk.base_model.model.norm.weight', 'model.image_encoder.trunk.base_model.model.norm.bias', 'model.image_encoder.head.mlp.fc1.weight', 'model.image_encoder.head.mlp.fc1.bias', 'model.image_encoder.head.mlp.norm.weight', 'model.image_encoder.head.mlp.norm.bias', 'model.image_encoder.head.mlp.fc2.weight', 'model.image_encoder.head.mlp.fc2.bias', 'model.gexp_encoder.trunk.trunk.pos', 'model.gexp_encoder.trunk.trunk.encoder_layer.self_attn.in_proj_weight', 'model.gexp_encoder.trunk.trunk.encoder_layer.self_attn.in_proj_bias', 'model.gexp_encoder.trunk.trunk.encoder_layer.self_attn.out_proj.weight', 'model.gexp_encoder.trunk.trunk.encoder_layer.self_attn.out_proj.bias', 'model.gexp_encoder.trunk.trunk.encoder_layer.linear1.weight', 'model.gexp_encoder.trunk.trunk.encoder_layer.linear1.bias', 'model.gexp_encoder.trunk.trunk.encoder_layer.linear2.weight', 'model.gexp_encoder.trunk.trunk.encoder_layer.linear2.bias', 'model.gexp_encoder.trunk.trunk.encoder_layer.norm1.weight', 'model.gexp_encoder.trunk.trunk.encoder_layer.norm1.bias', 'model.gexp_encoder.trunk.trunk.encoder_layer.norm2.weight', 'model.gexp_encoder.trunk.trunk.encoder_layer.norm2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.0.self_attn.in_proj_weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.0.self_attn.in_proj_bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.0.self_attn.out_proj.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.0.self_attn.out_proj.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.0.linear1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.0.linear1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.0.linear2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.0.linear2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.0.norm1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.0.norm1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.0.norm2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.0.norm2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.1.self_attn.in_proj_weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.1.self_attn.in_proj_bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.1.self_attn.out_proj.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.1.self_attn.out_proj.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.1.linear1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.1.linear1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.1.linear2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.1.linear2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.1.norm1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.1.norm1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.1.norm2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.1.norm2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.2.self_attn.in_proj_weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.2.self_attn.in_proj_bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.2.self_attn.out_proj.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.2.self_attn.out_proj.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.2.linear1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.2.linear1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.2.linear2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.2.linear2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.2.norm1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.2.norm1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.2.norm2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.2.norm2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.3.self_attn.in_proj_weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.3.self_attn.in_proj_bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.3.self_attn.out_proj.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.3.self_attn.out_proj.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.3.linear1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.3.linear1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.3.linear2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.3.linear2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.3.norm1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.3.norm1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.3.norm2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.3.norm2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.4.self_attn.in_proj_weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.4.self_attn.in_proj_bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.4.self_attn.out_proj.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.4.self_attn.out_proj.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.4.linear1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.4.linear1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.4.linear2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.4.linear2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.4.norm1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.4.norm1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.4.norm2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.4.norm2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.5.self_attn.in_proj_weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.5.self_attn.in_proj_bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.5.self_attn.out_proj.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.5.self_attn.out_proj.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.5.linear1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.5.linear1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.5.linear2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.5.linear2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.5.norm1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.5.norm1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.5.norm2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.5.norm2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.6.self_attn.in_proj_weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.6.self_attn.in_proj_bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.6.self_attn.out_proj.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.6.self_attn.out_proj.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.6.linear1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.6.linear1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.6.linear2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.6.linear2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.6.norm1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.6.norm1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.6.norm2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.6.norm2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.7.self_attn.in_proj_weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.7.self_attn.in_proj_bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.7.self_attn.out_proj.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.7.self_attn.out_proj.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.7.linear1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.7.linear1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.7.linear2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.7.linear2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.7.norm1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.7.norm1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.7.norm2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.7.norm2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.8.self_attn.in_proj_weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.8.self_attn.in_proj_bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.8.self_attn.out_proj.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.8.self_attn.out_proj.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.8.linear1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.8.linear1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.8.linear2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.8.linear2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.8.norm1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.8.norm1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.8.norm2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.8.norm2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.9.self_attn.in_proj_weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.9.self_attn.in_proj_bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.9.self_attn.out_proj.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.9.self_attn.out_proj.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.9.linear1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.9.linear1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.9.linear2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.9.linear2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.9.norm1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.9.norm1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.9.norm2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.9.norm2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.10.self_attn.in_proj_weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.10.self_attn.in_proj_bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.10.self_attn.out_proj.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.10.self_attn.out_proj.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.10.linear1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.10.linear1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.10.linear2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.10.linear2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.10.norm1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.10.norm1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.10.norm2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.10.norm2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.11.self_attn.in_proj_weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.11.self_attn.in_proj_bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.11.self_attn.out_proj.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.11.self_attn.out_proj.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.11.linear1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.11.linear1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.11.linear2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.11.linear2.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.11.norm1.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.11.norm1.bias', 'model.gexp_encoder.trunk.trunk.encoder.layers.11.norm2.weight', 'model.gexp_encoder.trunk.trunk.encoder.layers.11.norm2.bias', 'model.gexp_encoder.trunk.trunk.embeddings.weight', 'model.gexp_encoder.trunk.trunk.positional_embedding.weight', 'model.gexp_encoder.head.proj.weight', 'model.gexp_encoder.head.proj.bias'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab8a8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logit_scale', 'image_encoder.trunk.base_model.model.cls_token', 'image_encoder.trunk.base_model.model.pos_embed', 'image_encoder.trunk.base_model.model.patch_embed.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.patch_embed.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.patch_embed.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.patch_embed.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.0.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.0.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.0.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.0.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.0.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.0.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.0.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.0.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.0.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.0.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.0.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.0.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.0.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.0.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.0.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.0.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.0.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.0.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.1.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.1.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.1.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.1.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.1.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.1.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.1.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.1.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.1.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.1.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.1.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.1.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.1.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.1.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.1.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.1.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.1.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.1.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.2.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.2.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.2.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.2.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.2.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.2.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.2.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.2.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.2.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.2.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.2.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.2.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.2.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.2.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.2.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.2.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.2.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.2.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.3.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.3.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.3.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.3.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.3.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.3.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.3.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.3.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.3.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.3.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.3.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.3.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.3.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.3.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.3.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.3.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.3.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.3.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.4.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.4.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.4.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.4.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.4.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.4.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.4.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.4.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.4.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.4.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.4.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.4.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.4.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.4.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.4.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.4.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.4.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.4.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.5.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.5.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.5.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.5.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.5.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.5.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.5.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.5.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.5.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.5.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.5.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.5.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.5.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.5.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.5.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.5.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.5.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.5.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.6.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.6.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.6.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.6.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.6.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.6.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.6.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.6.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.6.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.6.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.6.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.6.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.6.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.6.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.6.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.6.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.6.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.6.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.7.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.7.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.7.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.7.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.7.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.7.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.7.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.7.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.7.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.7.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.7.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.7.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.7.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.7.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.7.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.7.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.7.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.7.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.8.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.8.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.8.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.8.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.8.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.8.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.8.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.8.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.8.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.8.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.8.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.8.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.8.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.8.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.8.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.8.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.8.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.8.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.9.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.9.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.9.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.9.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.9.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.9.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.9.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.9.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.9.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.9.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.9.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.9.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.9.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.9.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.9.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.9.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.9.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.9.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.10.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.10.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.10.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.10.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.10.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.10.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.10.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.10.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.10.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.10.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.10.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.10.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.10.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.10.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.10.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.10.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.10.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.10.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.11.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.11.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.11.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.11.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.11.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.11.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.11.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.11.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.11.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.11.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.11.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.11.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.11.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.11.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.11.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.11.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.11.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.11.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.12.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.12.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.12.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.12.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.12.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.12.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.12.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.12.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.12.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.12.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.12.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.12.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.12.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.12.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.12.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.12.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.12.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.12.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.13.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.13.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.13.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.13.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.13.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.13.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.13.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.13.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.13.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.13.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.13.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.13.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.13.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.13.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.13.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.13.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.13.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.13.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.14.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.14.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.14.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.14.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.14.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.14.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.14.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.14.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.14.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.14.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.14.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.14.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.14.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.14.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.14.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.14.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.14.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.14.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.15.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.15.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.15.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.15.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.15.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.15.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.15.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.15.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.15.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.15.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.15.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.15.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.15.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.15.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.15.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.15.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.15.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.15.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.16.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.16.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.16.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.16.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.16.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.16.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.16.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.16.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.16.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.16.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.16.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.16.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.16.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.16.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.16.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.16.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.16.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.16.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.17.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.17.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.17.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.17.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.17.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.17.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.17.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.17.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.17.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.17.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.17.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.17.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.17.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.17.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.17.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.17.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.17.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.17.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.18.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.18.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.18.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.18.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.18.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.18.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.18.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.18.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.18.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.18.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.18.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.18.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.18.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.18.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.18.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.18.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.18.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.18.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.19.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.19.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.19.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.19.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.19.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.19.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.19.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.19.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.19.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.19.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.19.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.19.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.19.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.19.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.19.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.19.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.19.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.19.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.20.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.20.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.20.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.20.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.20.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.20.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.20.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.20.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.20.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.20.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.20.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.20.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.20.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.20.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.20.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.20.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.20.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.20.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.21.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.21.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.21.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.21.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.21.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.21.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.21.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.21.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.21.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.21.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.21.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.21.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.21.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.21.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.21.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.21.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.21.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.21.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.22.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.22.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.22.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.22.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.22.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.22.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.22.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.22.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.22.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.22.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.22.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.22.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.22.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.22.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.22.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.22.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.22.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.22.ls2.gamma', 'image_encoder.trunk.base_model.model.blocks.23.norm1.weight', 'image_encoder.trunk.base_model.model.blocks.23.norm1.bias', 'image_encoder.trunk.base_model.model.blocks.23.attn.qkv.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.23.attn.qkv.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.23.attn.qkv.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.23.attn.qkv.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.23.attn.proj.base_layer.weight', 'image_encoder.trunk.base_model.model.blocks.23.attn.proj.base_layer.bias', 'image_encoder.trunk.base_model.model.blocks.23.attn.proj.lora_A.default.weight', 'image_encoder.trunk.base_model.model.blocks.23.attn.proj.lora_B.default.weight', 'image_encoder.trunk.base_model.model.blocks.23.ls1.gamma', 'image_encoder.trunk.base_model.model.blocks.23.norm2.weight', 'image_encoder.trunk.base_model.model.blocks.23.norm2.bias', 'image_encoder.trunk.base_model.model.blocks.23.mlp.fc1.weight', 'image_encoder.trunk.base_model.model.blocks.23.mlp.fc1.bias', 'image_encoder.trunk.base_model.model.blocks.23.mlp.fc2.weight', 'image_encoder.trunk.base_model.model.blocks.23.mlp.fc2.bias', 'image_encoder.trunk.base_model.model.blocks.23.ls2.gamma', 'image_encoder.trunk.base_model.model.norm.weight', 'image_encoder.trunk.base_model.model.norm.bias', 'image_encoder.head.mlp.fc1.weight', 'image_encoder.head.mlp.fc1.bias', 'image_encoder.head.mlp.norm.weight', 'image_encoder.head.mlp.norm.bias', 'image_encoder.head.mlp.fc2.weight', 'image_encoder.head.mlp.fc2.bias', 'gexp_encoder.trunk.trunk.pos', 'gexp_encoder.trunk.trunk.encoder_layer.self_attn.in_proj_weight', 'gexp_encoder.trunk.trunk.encoder_layer.self_attn.in_proj_bias', 'gexp_encoder.trunk.trunk.encoder_layer.self_attn.out_proj.weight', 'gexp_encoder.trunk.trunk.encoder_layer.self_attn.out_proj.bias', 'gexp_encoder.trunk.trunk.encoder_layer.linear1.weight', 'gexp_encoder.trunk.trunk.encoder_layer.linear1.bias', 'gexp_encoder.trunk.trunk.encoder_layer.linear2.weight', 'gexp_encoder.trunk.trunk.encoder_layer.linear2.bias', 'gexp_encoder.trunk.trunk.encoder_layer.norm1.weight', 'gexp_encoder.trunk.trunk.encoder_layer.norm1.bias', 'gexp_encoder.trunk.trunk.encoder_layer.norm2.weight', 'gexp_encoder.trunk.trunk.encoder_layer.norm2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.0.self_attn.in_proj_weight', 'gexp_encoder.trunk.trunk.encoder.layers.0.self_attn.in_proj_bias', 'gexp_encoder.trunk.trunk.encoder.layers.0.self_attn.out_proj.weight', 'gexp_encoder.trunk.trunk.encoder.layers.0.self_attn.out_proj.bias', 'gexp_encoder.trunk.trunk.encoder.layers.0.linear1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.0.linear1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.0.linear2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.0.linear2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.0.norm1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.0.norm1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.0.norm2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.0.norm2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.1.self_attn.in_proj_weight', 'gexp_encoder.trunk.trunk.encoder.layers.1.self_attn.in_proj_bias', 'gexp_encoder.trunk.trunk.encoder.layers.1.self_attn.out_proj.weight', 'gexp_encoder.trunk.trunk.encoder.layers.1.self_attn.out_proj.bias', 'gexp_encoder.trunk.trunk.encoder.layers.1.linear1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.1.linear1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.1.linear2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.1.linear2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.1.norm1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.1.norm1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.1.norm2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.1.norm2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.2.self_attn.in_proj_weight', 'gexp_encoder.trunk.trunk.encoder.layers.2.self_attn.in_proj_bias', 'gexp_encoder.trunk.trunk.encoder.layers.2.self_attn.out_proj.weight', 'gexp_encoder.trunk.trunk.encoder.layers.2.self_attn.out_proj.bias', 'gexp_encoder.trunk.trunk.encoder.layers.2.linear1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.2.linear1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.2.linear2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.2.linear2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.2.norm1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.2.norm1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.2.norm2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.2.norm2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.3.self_attn.in_proj_weight', 'gexp_encoder.trunk.trunk.encoder.layers.3.self_attn.in_proj_bias', 'gexp_encoder.trunk.trunk.encoder.layers.3.self_attn.out_proj.weight', 'gexp_encoder.trunk.trunk.encoder.layers.3.self_attn.out_proj.bias', 'gexp_encoder.trunk.trunk.encoder.layers.3.linear1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.3.linear1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.3.linear2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.3.linear2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.3.norm1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.3.norm1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.3.norm2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.3.norm2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.4.self_attn.in_proj_weight', 'gexp_encoder.trunk.trunk.encoder.layers.4.self_attn.in_proj_bias', 'gexp_encoder.trunk.trunk.encoder.layers.4.self_attn.out_proj.weight', 'gexp_encoder.trunk.trunk.encoder.layers.4.self_attn.out_proj.bias', 'gexp_encoder.trunk.trunk.encoder.layers.4.linear1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.4.linear1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.4.linear2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.4.linear2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.4.norm1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.4.norm1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.4.norm2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.4.norm2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.5.self_attn.in_proj_weight', 'gexp_encoder.trunk.trunk.encoder.layers.5.self_attn.in_proj_bias', 'gexp_encoder.trunk.trunk.encoder.layers.5.self_attn.out_proj.weight', 'gexp_encoder.trunk.trunk.encoder.layers.5.self_attn.out_proj.bias', 'gexp_encoder.trunk.trunk.encoder.layers.5.linear1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.5.linear1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.5.linear2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.5.linear2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.5.norm1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.5.norm1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.5.norm2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.5.norm2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.6.self_attn.in_proj_weight', 'gexp_encoder.trunk.trunk.encoder.layers.6.self_attn.in_proj_bias', 'gexp_encoder.trunk.trunk.encoder.layers.6.self_attn.out_proj.weight', 'gexp_encoder.trunk.trunk.encoder.layers.6.self_attn.out_proj.bias', 'gexp_encoder.trunk.trunk.encoder.layers.6.linear1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.6.linear1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.6.linear2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.6.linear2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.6.norm1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.6.norm1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.6.norm2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.6.norm2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.7.self_attn.in_proj_weight', 'gexp_encoder.trunk.trunk.encoder.layers.7.self_attn.in_proj_bias', 'gexp_encoder.trunk.trunk.encoder.layers.7.self_attn.out_proj.weight', 'gexp_encoder.trunk.trunk.encoder.layers.7.self_attn.out_proj.bias', 'gexp_encoder.trunk.trunk.encoder.layers.7.linear1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.7.linear1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.7.linear2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.7.linear2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.7.norm1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.7.norm1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.7.norm2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.7.norm2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.8.self_attn.in_proj_weight', 'gexp_encoder.trunk.trunk.encoder.layers.8.self_attn.in_proj_bias', 'gexp_encoder.trunk.trunk.encoder.layers.8.self_attn.out_proj.weight', 'gexp_encoder.trunk.trunk.encoder.layers.8.self_attn.out_proj.bias', 'gexp_encoder.trunk.trunk.encoder.layers.8.linear1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.8.linear1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.8.linear2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.8.linear2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.8.norm1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.8.norm1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.8.norm2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.8.norm2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.9.self_attn.in_proj_weight', 'gexp_encoder.trunk.trunk.encoder.layers.9.self_attn.in_proj_bias', 'gexp_encoder.trunk.trunk.encoder.layers.9.self_attn.out_proj.weight', 'gexp_encoder.trunk.trunk.encoder.layers.9.self_attn.out_proj.bias', 'gexp_encoder.trunk.trunk.encoder.layers.9.linear1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.9.linear1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.9.linear2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.9.linear2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.9.norm1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.9.norm1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.9.norm2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.9.norm2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.10.self_attn.in_proj_weight', 'gexp_encoder.trunk.trunk.encoder.layers.10.self_attn.in_proj_bias', 'gexp_encoder.trunk.trunk.encoder.layers.10.self_attn.out_proj.weight', 'gexp_encoder.trunk.trunk.encoder.layers.10.self_attn.out_proj.bias', 'gexp_encoder.trunk.trunk.encoder.layers.10.linear1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.10.linear1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.10.linear2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.10.linear2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.10.norm1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.10.norm1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.10.norm2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.10.norm2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.11.self_attn.in_proj_weight', 'gexp_encoder.trunk.trunk.encoder.layers.11.self_attn.in_proj_bias', 'gexp_encoder.trunk.trunk.encoder.layers.11.self_attn.out_proj.weight', 'gexp_encoder.trunk.trunk.encoder.layers.11.self_attn.out_proj.bias', 'gexp_encoder.trunk.trunk.encoder.layers.11.linear1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.11.linear1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.11.linear2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.11.linear2.bias', 'gexp_encoder.trunk.trunk.encoder.layers.11.norm1.weight', 'gexp_encoder.trunk.trunk.encoder.layers.11.norm1.bias', 'gexp_encoder.trunk.trunk.encoder.layers.11.norm2.weight', 'gexp_encoder.trunk.trunk.encoder.layers.11.norm2.bias', 'gexp_encoder.trunk.trunk.embeddings.weight', 'gexp_encoder.trunk.trunk.positional_embedding.weight', 'gexp_encoder.head.proj.weight', 'gexp_encoder.head.proj.bias'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_state_dict = {k.removeprefix(\"model.\"): v for k, v in state_dict.items()}\n",
    "cleaned_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26b4d707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "missing, unexpected = model.load_state_dict(cleaned_state_dict, strict=False)\n",
    "print(missing)\n",
    "print(unexpected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "238313d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageEncoder(\n",
       "  (trunk): PeftModel(\n",
       "    (base_model): LoraModel(\n",
       "      (model): VisionTransformer(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): lora.Conv2d(\n",
       "            (base_layer): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Conv2d(3, 8, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Conv2d(8, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (norm): Identity()\n",
       "        )\n",
       "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "        (patch_drop): Identity()\n",
       "        (norm_pre): Identity()\n",
       "        (blocks): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (4): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (5): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (6): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (7): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (8): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (9): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (10): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (11): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (12): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (13): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (14): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (15): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (16): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (17): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (18): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (19): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (20): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (21): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (22): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (23): Block(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (fc_norm): Identity()\n",
       "        (head_drop): Dropout(p=0.0, inplace=False)\n",
       "        (head): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (drop1): Dropout(p=0.2, inplace=False)\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (drop2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_encoder = model.image_encoder\n",
    "image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b574bf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.Tensor(8, 3, 224, 224).uniform_()\n",
    "output = image_encoder(dummy_input)\n",
    "print(output.shape)  # Output shape: [batch_size, num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2ceea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hescape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
